{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import requests_cache\n",
    "import lxml.html as lx\n",
    "from save_load import save_load\n",
    "save_load = save_load()\n",
    "\n",
    "requests_cache.install_cache(\"internal/mycache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Retrieval\n",
    "\n",
    "Example url used for read_html_table:\n",
    "\n",
    "https://www.arb.ca.gov/aqmis2/display.php?param=OZONE&units=007&year=2018&report=SITE1YR&statistic=DMAX&site=2460&ptype=aqd\n",
    "\n",
    "Example url used for read_links:\n",
    "\n",
    "https://www.arb.ca.gov/aqmis2/display.php?param=OZONE&units=007&year=2018&county_name=--COUNTY--&basin=--AIR+BASIN--&latitude=A-Whole+State&report=YRINV&order=s.name&submit=Retrieve+Data&ptype=aqd&std15=\n",
    "\n",
    "read_links(\"OZONE\", \"007\", 2005, \"https://www.arb.ca.gov/aqmis2/display.php?param=OZONE&units=007&year=2005&county_name=--COUNTY--&basin=--AIR+BASIN--&latitude=A-Whole+State&report=YRINV&order=s.name&submit=Retrieve+Data&ptype=aqd&std15=\")\n",
    "\n",
    "Example url used in get_all_links:\n",
    "\n",
    "https://www.arb.ca.gov/aqmis2/display.php?param=OZONE&units=007&year=2018&county_name=--COUNTY--&basin=--AIR+BASIN--&latitude=A-Whole+State&report=YRINV&order=s.name&submit=Retrieve+Data&ptype=aqd&std15="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_html_table(url):\n",
    "    \"\"\"\n",
    "    Given URL to one AQMIS page for a site, find the table that contains daily ozone for a given year,\n",
    "    (average ozone for each hour is collected, then the table is max ozone out of all hours in the day)\n",
    "    return a pandas data frame\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    html = response.content\n",
    "    df_list = pd.read_html(html)\n",
    "    # table with index 2 is what we want\n",
    "    return df_list[2]\n",
    "\n",
    "\n",
    "def reformat_table(df):\n",
    "    \"\"\"\n",
    "    Given a data frame taken from the page of one AQMIS site,\n",
    "    remove unneeded rows and columns and return it\n",
    "    \"\"\"\n",
    "    return df.drop([0, 1, 33, 34, 35]).drop([0], axis = 1)\n",
    "\n",
    "\n",
    "def read_links(param, units, year, url):\n",
    "    \"\"\"\n",
    "    Given a URL to the AQMIS site containing links to tables for all locations,\n",
    "    find all the links to the proper tables and return them in a list\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # xpath to find links in table data element\n",
    "    html = lx.fromstring(response.text)\n",
    "    links = html.xpath(\"//td/a\")\n",
    "    \n",
    "    links_list = []\n",
    "    param_check = \"\".join([\"param=\", param])\n",
    "    units_check = \"\".join([\"units=\", units])\n",
    "    year_check = \"\".join([\"year=\", str(year)])\n",
    "    report_check = \"report=SITE1YR\"\n",
    "    \n",
    "    # for every link element from the xpath query, find the actual text of the link and\n",
    "    # add it to the list if the link satistifies requirements for the link we want\n",
    "    for link_element in links:\n",
    "        link = link_element.attrib[\"href\"]\n",
    "        if param_check in link and units_check in link and year_check in link and report_check in link:\n",
    "            links_list.append(link)\n",
    "    return links_list\n",
    "\n",
    "\n",
    "def get_all_links(param, units, start_year, end_year):\n",
    "    \"\"\"\n",
    "    Given the parameter and the years, for each year, go to several AQMIS pages to retrieve the links to all sites,\n",
    "    return all the links in a list\n",
    "    \"\"\"\n",
    "    years = list(range(start_year, end_year + 1))\n",
    "    all_links = []\n",
    "    for year in years:\n",
    "        url = \"\".join([\"https://www.arb.ca.gov/aqmis2/display.php?param=\", param, \"&units=\", units, \"&year=\", str(year), \"&county_name=--COUNTY--&basin=--AIR+BASIN--&latitude=A-Whole+State&report=YRINV&order=s.name&submit=Retrieve+Data&ptype=aqd&std15=\"])\n",
    "        list_of_links = read_links(param, units, year, url)\n",
    "        for link in list_of_links:\n",
    "            all_links.append(link)\n",
    "    return all_links\n",
    "\n",
    "\n",
    "def get_all_tables(links):\n",
    "    \"\"\"\n",
    "    Given a list of AQMIS links, query the table from the link as a pandas data frame,\n",
    "    each table being the yearly table from one site, and return a list of data frames\n",
    "    \"\"\"\n",
    "    all_tables = []\n",
    "    for link in links:\n",
    "        url = \"\".join([\"https://www.arb.ca.gov/aqmis2/\", link])\n",
    "        table = read_html_table(url)\n",
    "        all_tables.append(table)\n",
    "    return all_tables\n",
    "\n",
    "\n",
    "def purge_bad_tables(list_of_tables):\n",
    "    \"\"\"\n",
    "    Given a list of tables, each table being the yearly table from one site,\n",
    "    remove the ones that do not have the right shape, return the list\n",
    "    \"\"\"\n",
    "    for i in range(len(list_of_tables) - 1, -1, -1):\n",
    "        if list_of_tables[i].shape != (36, 13):\n",
    "            del list_of_tables[i]\n",
    "    return list_of_tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OZONE tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ozone_links = get_all_links(\"OZONE\", \"007\", 2005, 2018)\n",
    "ozone_tables = get_all_tables(ozone_links)\n",
    "ozone_tables = purge_bad_tables(ozone_tables)\n",
    "save_load.save_object(ozone_tables, \"internal/ozone_tables.pkl\")\n",
    "ozone_tables = save_load.load_object(\"internal/ozone_tables.pkl\")\n",
    "\n",
    "save_load.save_object(ozone_links, \"internal/ozone_links.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PM2.5 tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pm25_links = get_all_links(\"PM25\", \"001\", 2005, 2018)\n",
    "pm25_tables = get_all_tables(pm25_links)\n",
    "pm25_tables = purge_bad_tables(pm25_tables)\n",
    "save_load.save_object(pm25_tables, \"internal/pm25_tables.pkl\")\n",
    "pm25_tables = save_load.load_object(\"internal/pm25_tables.pkl\")\n",
    "\n",
    "save_load.save_object(pm25_links, \"internal/pm25_links.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling (BY MONTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def flatten(table):\n",
    "    \"\"\"Given a table, return it in a row format\"\"\"\n",
    "    row = []\n",
    "    \n",
    "    # get location name\n",
    "    location_name = table[0][0].split(\"\\xa0\")[0].split(\" Daily\")[0]\n",
    "    row.append(location_name)\n",
    "    \n",
    "    # get year\n",
    "    year = table[0][0].split(\" Data\")[1].split(\" Parts\")[0].split(\" Micrograms\")[0]\n",
    "    row.append(int(year))\n",
    "    \n",
    "    # get average per month, blank if month is empty\n",
    "    for i in range(0, 12):\n",
    "        daily = reformat_table(table).iloc[:,i]\n",
    "        avgs = []\n",
    "        for item in daily:\n",
    "            if (str(item) != \"nan\"):\n",
    "                avgs.append(float(item))\n",
    "        if len(avgs) == 0:\n",
    "            row.append(\"\")\n",
    "        else:\n",
    "            row.append(np.mean(avgs))\n",
    "    return row\n",
    "\n",
    "\n",
    "def flatten_all(list_of_tables):\n",
    "    \"\"\"Given a list of tables, return a list of them flattened with flatten function\"\"\"\n",
    "    list_of_rows = []\n",
    "    for table in list_of_tables:\n",
    "        list_of_rows.append(flatten(table))\n",
    "    return list_of_rows\n",
    "\n",
    "\n",
    "def create_col_names(start_year, end_year):\n",
    "    \"\"\"Return a list of the column names: every month from 2005 to 2018\"\"\"\n",
    "    col_names = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in range(1, 13):\n",
    "            col_names.append(\"\".join([str(month), \"/\", str(year)]))\n",
    "    return(col_names)\n",
    "\n",
    "\n",
    "def assemble(list_of_tables):\n",
    "    \"\"\"Given a list of tables, assemble the entire table\"\"\"\n",
    "    # flatten, create location and month.year row and column names\n",
    "    list_of_rows = flatten_all(list_of_tables)\n",
    "    locations = pd.unique(pd.DataFrame(list_of_rows).iloc[:,0])\n",
    "    col_names = create_col_names(2005, 2018)\n",
    "    \n",
    "    # fill up empty data frame with elements in list of rows\n",
    "    df = pd.DataFrame(index = locations, columns = col_names)\n",
    "    for row in list_of_rows:\n",
    "        row_index = row[0]\n",
    "        for i in range(1, 13):\n",
    "            col_index = \"\".join([str(i), \"/\", str(row[1])])\n",
    "            df.loc[row_index].loc[col_index] = row[1 + i]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OZONE full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ozone = assemble(ozone_tables)\n",
    "ozone.to_excel(\"datasets/ozone_month.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PM2.5 full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25 = assemble(pm25_tables)\n",
    "pm25.to_excel(\"datasets/pm25_month.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
